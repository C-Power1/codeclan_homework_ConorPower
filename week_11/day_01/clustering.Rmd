---
title: "R Notebook"
output: html_notebook
---

# Loading libraries

```{r}
library(tidyverse)
library(cluster)
library(factoextra)
library(dendextend)
library(broom)
library(animation)
library(janitor)
library(cluster)
```

# Reading, exploring and plotting data 

```{r}
computers <- read_csv("data/computers.csv")

summary(computers)

```

```{r}
computers <- computers %>% 
  select(hd, ram)
```

```{r}
ggplot(computers) +
  aes(x = ram, y = hd) +
  geom_point()
```

After exploring and plotting the data it can be said that the dataset is suitable for clustering analysis as there appears to some well-defined groupings - looks to be 2/3 clear groupings. As the goal is to find patterns between the two variables, k-means is the best clustering analysis option for this.

# Scaling the data 

```{r}
computers_scale <- computers %>% 
   mutate_all(scale)
```

# Choosing a k value

3 choosing methods (elbow, silhouette and gap) will be used to determine the k value (most common value will be the chosen k value)

## Elbow

```{r}
fviz_nbclust(computers_scale, kmeans, method = "wss", nstart = 25)
```

## Silhouette 

```{r}
fviz_nbclust(computers_scale, kmeans, method = "silhouette", nstart = 25)
```

#Gap

```{r}
fviz_nbclust(computers_scale, kmeans, method = "gap_stat", nstart = 25, k.max = 10)
```

Gap doesn't seem to be working

The elbow method shows 2 to be the optimal k value while the silhouette method shows 10 to be the optimal (although 2 and 7 looks to meet at the same y value). As the gap method does not appear to work, based on the plots above and the original computers plot, 2 is selected as the k value.

# Visualising the clusters

```{r}
clustered_computers <- kmeans(computers_scale,
                            centers = 2,
                            nstart = 25)
```

```{r}
computers_scale %>% 
  kmeans.ani(centers = 2)
```

```{r}
max_k <- 20 

k_clusters <- tibble(k = 1:max_k) %>%
  mutate(
    kclust = map(k, ~ kmeans(computers_scale, .x, nstart = 25)), 
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, computers)
  )


clusterings <- k_clusters %>%
  unnest(glanced)

clusterings
```

```{r}
 clusterings %>% 
  unnest(cols = c(augmented)) %>%
  filter(k == 2) %>%
  ggplot(aes(x = ram, y = hd, colour = .cluster)) +
  geom_point(aes(color = .cluster))
```

```{r}
 clusterings %>% 
  unnest(augmented) %>%
  filter(k == 2) %>%
  group_by(.cluster) %>%
  summarise(mean(ram), mean(hd))
```


I think the clustering worked to an extent. The data when plotted originally showed potential for 2/3 clusters and further analysis showed that 2 clusters were likely to be present within the data. This is further supported when looking at the mean values for each cluster - it is clear there is a distinct difference between the two. However, there appears to be several outliers within the data so maybe utlising k-medoids clustering analysis would be a better option.





















